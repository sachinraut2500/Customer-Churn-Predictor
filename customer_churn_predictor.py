# -*- coding: utf-8 -*-
"""Customer Churn Predictor

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1a-DaSwyTIVpHrugM4-3iD72lN-5X1nEi
"""

# =========================
# requirements.txt
# =========================
pandas>=1.5
numpy>=1.24
scikit-learn>=1.1
xgboost>=1.7
joblib
shap>=0.41
tqdm

# =========================
# repo layout
# =========================
# churn-predictor/
# ├─ data/
# │  └─ raw/                # customers.csv (contains label 'churn' 0/1)
# ├─ src/
# │  ├─ __init__.py
# │  ├─ preprocess.py       # cleaning & feature engineering
# │  ├─ train.py            # training pipeline (XGBoost)
# │  ├─ explain.py          # SHAP explanations for individual customers
# │  └─ utils.py            # small helpers
# └─ requirements.txt

# =========================
# src/__init__.py
# =========================
# empty

# =========================
# src/utils.py
# =========================
from pathlib import Path
import pandas as pd
import numpy as np

def load_csv(path: Path) -> pd.DataFrame:
    return pd.read_csv(path)

def save_model(model, path: Path):
    import joblib
    path.parent.mkdir(parents=True, exist_ok=True)
    joblib.dump(model, path)

def load_model(path: Path):
    import joblib
    return joblib.load(path)

# =========================
# src/preprocess.py
# =========================
"""
preprocess.py
- Loads a customer CSV and performs common preprocessing:
  - fills missing values
  - encodes categorical variables (simple one-hot / label)
  - returns train-ready CSV or DataFrame with features + target 'churn'
Expected input columns minimally: customer_id (optional), churn (0/1), plus other features.
"""
from pathlib import Path
import pandas as pd
import numpy as np

def basic_clean(df: pd.DataFrame) -> pd.DataFrame:
    # drop duplicate rows
    df = df.drop_duplicates().reset_index(drop=True)
    # fill numeric NaNs with median
    num_cols = df.select_dtypes(include=["number"]).columns.tolist()
    for c in num_cols:
        df[c] = df[c].fillna(df[c].median())
    # fill object NaNs with 'unknown'
    obj_cols = df.select_dtypes(include=["object", "category"]).columns.tolist()
    for c in obj_cols:
        df[c] = df[c].fillna("unknown")
    return df

def encode_features(df: pd.DataFrame, categorical_threshold: int = 20) -> pd.DataFrame:
    df = df.copy()
    # find categorical columns excluding 'churn'
    cats = [c for c in df.select_dtypes(include=["object", "category"]).columns if c != "churn"]
    for c in cats:
        if df[c].nunique() <= categorical_threshold:
            dummies = pd.get_dummies(df[c], prefix=c, drop_first=True)
            df = pd.concat([df.drop(columns=[c]), dummies], axis=1)
        else:
            # high-cardinality -> label encode
            df[c] = df[c].astype("category").cat.codes
    return df

def preprocess(input_csv: Path, out_csv: Path):
    df = pd.read_csv(input_csv)
    if "churn" not in df.columns:
        raise ValueError("Input CSV must include a 'churn' column with 0/1 values.")
    df = basic_clean(df)
    df = encode_features(df)
    df.to_csv(out_csv, index=False)
    print(f"Wrote processed data to {out_csv} ({len(df)} rows)")

if __name__ == "__main__":
    import argparse
    parser = argparse.ArgumentParser()
    parser.add_argument("--input", required=True, help="Raw customers.csv")
    parser.add_argument("--out", required=True, help="Processed CSV output")
    args = parser.parse_args()
    preprocess(Path(args.input), Path(args.out))

# =========================
# src/train.py
# =========================
"""
train.py
- Loads processed CSV (from preprocess.py)
- Trains an XGBoost classifier with simple train/validation split
- Saves model artifact and metadata (feature list, metrics)
"""
from pathlib import Path
import pandas as pd
import numpy as np
import json
from sklearn.model_selection import train_test_split
from sklearn.metrics import roc_auc_score, accuracy_score
import xgboost as xgb
from datetime import datetime
import joblib

def load_processed(path: Path) -> pd.DataFrame:
    return pd.read_csv(path)

def train_model(df: pd.DataFrame, target="churn", test_size=0.15, random_state=42):
    features = [c for c in df.columns if c != target and c != "customer_id"]
    X = df[features]
    y = df[target].astype(int)
    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=test_size, random_state=random_state, stratify=y)
    model = xgb.XGBClassifier(
        n_estimators=200, max_depth=6, learning_rate=0.1, use_label_encoder=False, eval_metric="logloss", random_state=random_state
    )
    model.fit(X_train, y_train, early_stopping_rounds=20, eval_set=[(X_val, y_val)], verbose=False)
    val_preds = model.predict_proba(X_val)[:,1]
    val_labels = model.predict(X_val)
    auc = roc_auc_score(y_val, val_preds)
    acc = accuracy_score(y_val, val_labels)
    return model, {"auc": float(auc), "accuracy": float(acc), "features": features}

def save_artifacts(model, metadata: dict, model_path: Path):
    model_path.parent.mkdir(parents=True, exist_ok=True)
    joblib.dump(model, model_path)
    with open(str(model_path) + ".meta.json", "w") as f:
        json.dump(metadata, f, indent=2)
    print(f"Saved model to {model_path} and metadata.")

def main(processed_csv: Path, out_model: Path):
    df = load_processed(processed_csv)
    model, metrics = train_model(df)
    metadata = {
        "trained_at": datetime.utcnow().isoformat() + "Z",
        "metrics": metrics
    }
    save_artifacts(model, metadata, out_model)
    print(f"Training complete. AUC={metrics['auc']:.4f}, Accuracy={metrics['accuracy']:.4f}")

if __name__ == "__main__":
    import argparse
    parser = argparse.ArgumentParser()
    parser.add_argument("--processed", required=True)
    parser.add_argument("--out", default="models/churn_xgb.joblib")
    args = parser.parse_args()
    main(Path(args.processed), Path(args.out))

# =========================
# src/explain.py
# =========================
"""
explain.py
- Load model and a single-customer sample (CSV or JSON) and produce SHAP explanation values.
- Outputs a small JSON with top positive and negative feature contributions.
"""
from pathlib import Path
import pandas as pd
import numpy as np
import json
import shap
import joblib

def load_model_and_meta(model_path: Path):
    model = joblib.load(model_path)
    meta_path = Path(str(model_path) + ".meta.json")
    meta = {}
    if meta_path.exists():
        meta = json.load(open(meta_path))
    return model, meta

def explain(model_path: Path, sample_path: Path, background_path: Path = None, top_k: int = 10):
    model, meta = load_model_and_meta(model_path)
    sample = pd.read_csv(sample_path) if sample_path.suffix == ".csv" else pd.read_json(sample_path, orient="records")
    if background_path:
        background = pd.read_csv(background_path)
        bg = background.sample(min(100, len(background)), random_state=0)
    else:
        # if no background provided, create a small synthetic background from sample features (not ideal)
        bg = sample
    # ensure sample has the same columns as training features
    features = meta.get("metrics", {}).get("features") or meta.get("features")
    if features:
        X_sample = sample[features]
        X_bg = bg[features]
    else:
        X_sample = sample.select_dtypes(include=[np.number])
        X_bg = bg.select_dtypes(include=[np.number])
    explainer = shap.Explainer(model, X_bg)
    shap_values = explainer(X_sample)
    out = []
    for i in range(len(X_sample)):
        vals = shap_values[i]
        arr = list(zip(vals.feature_names, vals.values.tolist()))
        # sort by absolute impact
        arr_sorted = sorted(arr, key=lambda x: abs(x[1]), reverse=True)[:top_k]
        pos = [ {"feature": f, "shap_value": float(v)} for f,v in arr_sorted if v>0 ]
        neg = [ {"feature": f, "shap_value": float(v)} for f,v in arr_sorted if v<0 ]
        out.append({"index": int(X_sample.index[i]), "positive": pos, "negative": neg})
    return out

if __name__ == "__main__":
    import argparse
    parser = argparse.ArgumentParser()
    parser.add_argument("--model", required=True, help="Trained model path (joblib)")
    parser.add_argument("--sample", required=True, help="Single-row CSV/JSON with same features used in training")
    parser.add_argument("--background", default=None, help="CSV to use as background for SHAP (optional)")
    parser.add_argument("--out", default="explanations.json", help="Output JSON file")
    args = parser.parse_args()
    explanations = explain(Path(args.model), Path(args.sample), Path(args.background) if args.background else None)
    with open(args.out, "w") as f:
        json.dump(explanations, f, indent=2)
    print(f"Wrote explanations to {args.out}")